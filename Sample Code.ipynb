{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Pakages\" data-toc-modified-id=\"Pakages-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Pakages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normally-used-packages\" data-toc-modified-id=\"Normally-used-packages-0.1.1\"><span class=\"toc-item-num\">0.1.1&nbsp;&nbsp;</span>Normally used packages</a></span></li><li><span><a href=\"#Suppress-Warnings\" data-toc-modified-id=\"Suppress-Warnings-0.1.2\"><span class=\"toc-item-num\">0.1.2&nbsp;&nbsp;</span>Suppress Warnings</a></span></li><li><span><a href=\"#Machine-Learning-Packages\" data-toc-modified-id=\"Machine-Learning-Packages-0.1.3\"><span class=\"toc-item-num\">0.1.3&nbsp;&nbsp;</span>Machine Learning Packages</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Manipulation\" data-toc-modified-id=\"Data-Manipulation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Manipulation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discern-categorical-variable-and-continuous-variable\" data-toc-modified-id=\"Discern-categorical-variable-and-continuous-variable-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Discern categorical variable and continuous variable</a></span></li><li><span><a href=\"#Missing-Value\" data-toc-modified-id=\"Missing-Value-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Missing Value</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-missing-value-table\" data-toc-modified-id=\"Create-missing-value-table-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Create missing value table</a></span></li><li><span><a href=\"#Create-missing-value-chart\" data-toc-modified-id=\"Create-missing-value-chart-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Create missing value chart</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Visualization\" data-toc-modified-id=\"Data-Visualization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Visualization</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Plot-Categorical-Variabels\" data-toc-modified-id=\"Plot-Categorical-Variabels-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Plot Categorical Variabels</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stacked-Bar-Chart\" data-toc-modified-id=\"Stacked-Bar-Chart-2.0.1.1\"><span class=\"toc-item-num\">2.0.1.1&nbsp;&nbsp;</span>Stacked Bar Chart</a></span></li><li><span><a href=\"#Count-Plot\" data-toc-modified-id=\"Count-Plot-2.0.1.2\"><span class=\"toc-item-num\">2.0.1.2&nbsp;&nbsp;</span>Count Plot</a></span></li></ul></li><li><span><a href=\"#Check-the-pattern-differences-between-training-data-and-testing-data\" data-toc-modified-id=\"Check-the-pattern-differences-between-training-data-and-testing-data-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Check the pattern differences between training data and testing data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bar-Chart---categorical-variable\" data-toc-modified-id=\"Bar-Chart---categorical-variable-2.0.2.1\"><span class=\"toc-item-num\">2.0.2.1&nbsp;&nbsp;</span>Bar Chart - categorical variable</a></span></li><li><span><a href=\"#Violin-Chart---continuous-variable\" data-toc-modified-id=\"Violin-Chart---continuous-variable-2.0.2.2\"><span class=\"toc-item-num\">2.0.2.2&nbsp;&nbsp;</span>Violin Chart - continuous variable</a></span></li></ul></li><li><span><a href=\"#Heatmap---check-correlation\" data-toc-modified-id=\"Heatmap---check-correlation-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Heatmap - check correlation</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Scaling-or-Standardization\" data-toc-modified-id=\"Feature-Scaling-or-Standardization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Feature Scaling or Standardization</a></span></li><li><span><a href=\"#Label-Encoding\" data-toc-modified-id=\"Label-Encoding-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Label Encoding</a></span><ul class=\"toc-item\"><li><span><a href=\"#label-encoding-map\" data-toc-modified-id=\"label-encoding-map-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>label encoding map</a></span></li></ul></li><li><span><a href=\"#One-Hot-Encoding\" data-toc-modified-id=\"One-Hot-Encoding-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>One Hot Encoding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pandas-pd.getdummies\" data-toc-modified-id=\"Pandas-pd.getdummies-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Pandas pd.getdummies</a></span></li><li><span><a href=\"#Sk-Learn's-OneHotEncoder\" data-toc-modified-id=\"Sk-Learn's-OneHotEncoder-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Sk Learn's OneHotEncoder</a></span></li></ul></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ensemble\" data-toc-modified-id=\"Ensemble-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Ensemble</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ensemble-Technique-(En_optA)\" data-toc-modified-id=\"Ensemble-Technique-(En_optA)-4.1.1.1\"><span class=\"toc-item-num\">4.1.1.1&nbsp;&nbsp;</span>Ensemble Technique (En_optA)</a></span></li><li><span><a href=\"#Ensemble-Technique-(En_optB)\" data-toc-modified-id=\"Ensemble-Technique-(En_optB)-4.1.1.2\"><span class=\"toc-item-num\">4.1.1.2&nbsp;&nbsp;</span>Ensemble Technique (En_optB)</a></span></li></ul></li><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Create-dataset\" data-toc-modified-id=\"Create-dataset-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Create dataset</a></span></li><li><span><a href=\"#Layers\" data-toc-modified-id=\"Layers-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-Layer\" data-toc-modified-id=\"First-Layer-4.1.4.1\"><span class=\"toc-item-num\">4.1.4.1&nbsp;&nbsp;</span>First Layer</a></span></li><li><span><a href=\"#Second-Layer\" data-toc-modified-id=\"Second-Layer-4.1.4.2\"><span class=\"toc-item-num\">4.1.4.2&nbsp;&nbsp;</span>Second Layer</a></span></li></ul></li></ul></li><li><span><a href=\"#XGB\" data-toc-modified-id=\"XGB-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>XGB</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-model\" data-toc-modified-id=\"Basic-model-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Basic model</a></span></li><li><span><a href=\"#Monitor-the-Process\" data-toc-modified-id=\"Monitor-the-Process-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Monitor the Process</a></span></li><li><span><a href=\"#Feature-Importance\" data-toc-modified-id=\"Feature-Importance-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Feature Importance</a></span></li><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Hyperparameters</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pakages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normally used packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppress Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discern categorical variable and continuous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disce_catV_conV(df, classes):\n",
    "    \"\"\"\n",
    "    Discern categorical variable and continuous variable in a dataframe.\n",
    "    df: dataframe\n",
    "    classes: choose the threshold to discern variables\n",
    "    \n",
    "    return two lists:\n",
    "    1. cat_v: a list of categorical variables\n",
    "    2. con_v: a list of continuous variables\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # create two empty lists to put columsn\n",
    "    cat_v = []\n",
    "    con_v = []\n",
    "\n",
    "    for column in df.columns:\n",
    "\n",
    "        if len(df[column].value_counts().index) <= classes:\n",
    "            cat_v.append(column)\n",
    "        else:\n",
    "            con_v.append(column)\n",
    "            \n",
    "    print(\"The continuous variables: \", con_v, \"\\n\")\n",
    "    print(\"The categorical variables: \", cat_v)\n",
    "            \n",
    "    return cat_v, con_v\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create missing value table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "    \n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum().sort_values(ascending=False).round(1)\n",
    "    # Exclude columns with no missing values\n",
    "    mis_val = mis_val[mis_val.values != 0]\n",
    "   \n",
    "    #Percentage of missing values\n",
    "    mis_val_percent = 100 * mis_val / len(df)\n",
    "    \n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    \n",
    "    # Rename columns\n",
    "    mis_val_table = mis_val_table.rename(columns={0:'Missing Values', 1:'% of Missing Values'})\n",
    "    \n",
    "    # total col and # of col with missing values\n",
    "    total_col = df.shape[1]\n",
    "    missing_val_col = len(mis_val_table)\n",
    "    \n",
    "    print(\"Among \" + str(total_col) + \"columns, there are \" + \\\n",
    "          str(missing_val_col) + \" columns with missing values.\")\n",
    "    \n",
    "    \n",
    "    return mis_val_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create missing value chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963024</td>\n",
       "      <td>0.040390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.539205</td>\n",
       "      <td>0.297834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.595585</td>\n",
       "      <td>0.258728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234727</td>\n",
       "      <td>0.358745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042992</td>\n",
       "      <td>0.755945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.513842</td>\n",
       "      <td>0.746963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.615920</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.085843</td>\n",
       "      <td>0.842204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950184</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1   2         3         4\n",
       "0  0.963024  0.040390 NaN  0.539205  0.297834\n",
       "1  0.595585  0.258728 NaN  0.234727  0.358745\n",
       "2  0.042992  0.755945 NaN  0.272414       NaN\n",
       "3  0.513842  0.746963 NaN  0.615920       NaN\n",
       "4  0.085843  0.842204 NaN  0.950184       NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "arra = np.random.rand(5,5)\n",
    "arra[:, 2] = np.nan\n",
    "arra[2:5, 4] = np.nan\n",
    "\n",
    "df = pd.DataFrame(arra)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x200f48210f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import missingno\n",
    "missingno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Categorical Variabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_catVar(df, classes, target_col):\n",
    "    \"\"\"\n",
    "    Plot stacked bar chart for categorical variabels in order to compare the impact of classes to target value.\n",
    "    \n",
    "    parameters\n",
    "    -----------\n",
    "    df: dataframe\n",
    "    classes: Int\n",
    "        set a threshold to decide which are categorical variables\n",
    "    target_col: string\n",
    "        the column name we want to predict\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    graphs\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from matplotlib.colors import Colormap\n",
    "    \n",
    "    # create a list of all categorical variables\n",
    "    cat_V = []\n",
    "    for column in df.columns:\n",
    "        \n",
    "        if len(df[column].value_counts().index) >= classes:\n",
    "            cat_V.append(column)\n",
    "    \n",
    "    if target_col in cat_V:\n",
    "        cat_V.remove(target_col)\n",
    "    \n",
    "    # plot graphs\n",
    "    fig, ax = plt.subplots(nrows = len(cat_V), ncols = 1, figsize=(8,3 * len(cat_V)))\n",
    "    \n",
    "    for i, col in enumerate(cat_V):\n",
    "        cross_tab = pd.crosstab(df[col], df[target_col]).apply(lambda x: x/x.sum(), axis = 1)\n",
    "        cross_tab.plot(kind='bar', stacked =True, ax = ax[i], legend=False, colormap = 'tab20c')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    ax[0].legend(loc='upper right', prop={'size':15})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_countplot(df, num_rows, num_cols, fig_width, fig_length):\n",
    "    \n",
    "    '''\n",
    "    Create count plot for categorical variables.\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    df: dataframe\n",
    "    num_rows: int\n",
    "        number of rows for the chart; used in subplots(nrows = num_rows)\n",
    "    num_cols: int\n",
    "        number of columns for the chart; used in subplots(ncols = num_rows)\n",
    "    fig_width: int\n",
    "        figure width\n",
    "    fig_length: int\n",
    "        figure length\n",
    "    \n",
    "    return\n",
    "    -------\n",
    "    graphs\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # create a list of all categorical variables\n",
    "    cat_V = df.select_dtypes('object').columns\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        fg, ax = plt.subplots(nrows=1, ncols = num_cols, figsize = (fig_length, fig_width))\n",
    "        for j in range(num_cols):\n",
    "            sns.countplot(cat_V[i*num_cols + j], data=df, ax=ax[j])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the pattern differences between training data and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Chart - categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_diff_trainNtest(train, test, classes, target_col):\n",
    "    \n",
    "    '''\n",
    "    Will create pairs of bar chart to check the pattern differences between the training data and testing data\n",
    "    train: df\n",
    "    training dataset\n",
    "    \n",
    "    test: df\n",
    "    testing dataset\n",
    "    \n",
    "    classes: Int\n",
    "    set a threshold to decide which are categorical variables\n",
    "    \n",
    "    target_col: object\n",
    "    the column name we want to predict\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    # create a list of all categorical variables\n",
    "    cat_V = []\n",
    "    for column in train.columns:\n",
    "        if len(train[column].value_counts().index) <= classes:\n",
    "            cat_V.append(column)\n",
    "    if target_col in cat_V:\n",
    "        cat_V.remove(target_col)\n",
    "        \n",
    "    count = 1\n",
    "    \n",
    "    for i in range(len(cat_V)):\n",
    "        fig = plt.figure(figsize = (14, 3*len(cat_V)))\n",
    "        \n",
    "        plt.subplot(len(cat_V), 2, count)\n",
    "        plt.bar(train[cat_V[i]].value_counts().index, train[cat_V[i]].value_counts().values, color='darkcyan')\n",
    "        plt.title(\"Train: \" + cat_V[i])\n",
    "        \n",
    "        plt.subplot(len(cat_V), 2, count + 1)\n",
    "        plt.bar(test[cat_V[i]].value_counts().index, test[cat_V[i]].value_counts().values, color='powderblue')\n",
    "        plt.title(\"test: \" + cat_V[i])        \n",
    "        \n",
    "        count += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin Chart - continuous variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Violin plot is useful for visualizing continuous variables as it is a combination of box and density plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare continuous variables between train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_diff_trainNtest_vio(train, test, classes, target_col):\n",
    "    \n",
    "    '''\n",
    "    Will create pairs of violin chart to check the continuous number differences between the training data and testing data\n",
    "    train: df\n",
    "    training dataset\n",
    "    \n",
    "    test: df\n",
    "    testing dataset\n",
    "    \n",
    "    classes: Int\n",
    "    set a threshold to decide which are categorical variables\n",
    "    \n",
    "    target_col: object\n",
    "    the column name we want to predict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # create a list of all categorical variables\n",
    "    con_V = []\n",
    "    for column in train.columns:\n",
    "        if len(train[column].value_counts().index) >= classes:\n",
    "            con_V.append(column)\n",
    "\n",
    "        \n",
    "    count = 1\n",
    "    \n",
    "    for i in range(len(con_V)):\n",
    "        fig = plt.figure(figsize = (14, 3*len(con_V)))\n",
    "        \n",
    "        plt.subplot(len(con_V), 2, count)\n",
    "        plt.violinplot(train[con_V[i]], showmeans=True)\n",
    "        plt.title(\"Train: \" + con_V[i])\n",
    "        \n",
    "        plt.subplot(len(con_V), 2, count + 1)\n",
    "        plt.violinplot(test[con_V[i]], showmeans=True)\n",
    "        plt.title(\"test: \" + con_V[i])        \n",
    "        \n",
    "        count += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check each continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_violinplot(df, num_rows, num_cols, fig_width, fig_length):\n",
    "    \n",
    "    '''\n",
    "    Create count plot for categorical variables.\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    df: dataframe\n",
    "    num_rows: int\n",
    "        number of rows for the chart; used in subplots(nrows = num_rows)\n",
    "    num_cols: int\n",
    "        number of columns for the chart; used in subplots(ncols = num_rows)\n",
    "    fig_width: int\n",
    "        figure width\n",
    "    fig_length: int\n",
    "        figure length\n",
    "    \n",
    "    return\n",
    "    -------\n",
    "    graphs\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # create a list of all categorical variables\n",
    "    con_V = df.select_dtypes(['int64', 'float64']).columns\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        fg, ax = plt.subplots(nrows=1, ncols = num_cols, figsize = (fig_length, fig_width))\n",
    "        for j in range(num_cols):\n",
    "            sns.countplot(con_V[i*num_cols + j], data=df, ax=ax[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap - check correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df, fig_length=20, fit_width=15, vmin=-1, vmax=1, center = 0, annot=True):\n",
    "    '''\n",
    "    create a heatmap to check correlation\n",
    "    \n",
    "    df: dataframe\n",
    "    size: figure size\n",
    "    \n",
    "    '''   \n",
    "    corr = df.corr()\n",
    "    fig = plt.figure(figsize=(fig_length,fit_width))    \n",
    "    sns.heatmap(corr, vmin=-1, vmax=1, center=0,annot=True, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling or Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a step of data preprocessing. It helps to normalize the data within a particular range, which might \n",
    "help in speeding up the calculations in an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formula: z = (x - u)/z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "scalar.fit(data) #  this computes the mean and std to be used for later scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = LabelEncoder()\n",
    "LE.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encoding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = LabelEncoder()\n",
    "\n",
    "def le_map(df):\n",
    "    '''Label Enoding specifics for each column'''\n",
    "    LE_map = dict()\n",
    "    # select all categorical columns \n",
    "    cat_V = df.select_dtypes('category').columns\n",
    "    # fit_transform data and create label encoding map\n",
    "    for col in cat_V:\n",
    "        df[col] = LE.fit_transform(df[col])\n",
    "        LE_map[col] = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
    "    # return map\n",
    "    return LE_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OHE can be done by either Pandas' get_dummies() or SK Learn's OneHotEncoder. \n",
    "\n",
    "* get_dummies is easier to implement (can be used directly on raw categorical features, i.e. strings), but it takes longer time and is not memory efficient.\n",
    "\n",
    "* OneHotEncoder requires the features being converted to numeric, which has already been done by LabelEncoder in previous step, and is much more efficient (7x faster).\n",
    "\n",
    "* We will convert the OHE's results to a sparse matrix which uses way less memory as compared to dense matrix. However, not all algorithms and packagers support sparse matrix, e.g. Keras. In that case, we'll need to use other tricks to make it work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas pd.getdummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.getdummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sk Learn's OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE = OneHotEncoder(sparse = True) # sparse = True, means it will be creating a spared matrix, which saves memory.\n",
    "OHE.fit_transform(df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/svpons/three-level-classification-architecture\n",
    "# Like staking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning architecture:\n",
    "    \n",
    "* First layer: I am using 6 classifiers from scikit-learn (Support_Vector_Machines, Logistic_Regression, Random_Forest, Gradient_Boosting, Extra_Trees_Classifier, K_Nearest_Neighbors). All classifiers are used with (almost) default parameters. At this level, many other classifiers can be used. All classifiers are applied twice:\n",
    "           1. Classifiers are trained on (X_train, y_train) and used to predict the class probabilities of (X_valid).\n",
    "           2. Classifiers are trained on (X = (X_train + X_valid), y = (y_train + y_valid)) and used to predict the class probabilities of (X_test)\n",
    "* Second layer: The predictions from the previous layer on X_valid are concatenated and used to create a new training set (XV, y_valid). The predictions on X_test are concatenated to create a new test set (XT, y_test). The two proposed ensemble methods (EN_optA and EN_optB) and their calibrated versions are trained on (XV, y_valid) and used to predict the class probabilites of (XT).\n",
    "* Third layer: The four prediction from the previous layer are linearly combined using fixed weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Technique (En_optA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of predictions  X1,X2,...,Xn , it computes the optimal set of weights  w1,w2,...,wn ; such that minimizes  log_loss(yT,yE) , where  yE=X1∗w1+X2∗w2+...+Xn∗wn  and  yT  is the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objf_ens_optA(w, Xs, y, n_class=12):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class=12):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Technique (En_optB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of predictions  X1,X2,...,Xn , where each  Xi  has  m=12  clases, i.e.  Xi=Xi1,Xi2,...,Xim . The algorithm finds the optimal set of weights  w11,w12,...,wnm ; such that minimizes  log_loss(yT,yE) , where  yE=X11∗w11+...+X21∗w21+...+Xnm∗wnm  and and  yT  is the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objf_ens_optB(w, Xs, y, n_class=12):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class=12):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "#fixing random state\n",
    "random_state=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:\n",
      "X_train: (1200, 100), X_valid: (400, 100), X_test: (400, 100) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_classes = 12\n",
    "\n",
    "data, labels = make_classification(n_samples=2000, n_features=100,\n",
    "                                  n_informative = 50, n_classes = n_classes,\n",
    "                                  random_state = random_state)\n",
    "\n",
    "#Spliting data into train and test sets.\n",
    "X, X_test, y, y_test = train_test_split(data, labels, test_size=0.2, \n",
    "                                        random_state=random_state)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, \n",
    "                                                      random_state=random_state)\n",
    "\n",
    "print('Data shape:')\n",
    "print('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape, \n",
    "                                                  X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------\n",
      "LR:        logloss  => 2.1234358\n",
      "SVM:       logloss  => 2.4884455\n",
      "RF:        logloss  => 2.2804603\n",
      "GBM:       logloss  => 2.2861646\n",
      "ETC:       logloss  => 2.2718071\n",
      "KNN:       logloss  => 2.4560098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defining the classifiers\n",
    "\n",
    "clfs = {'LR'  : LogisticRegression(random_state=random_state), \n",
    "        'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1, \n",
    "                                       random_state=random_state), \n",
    "        'GBM' : GradientBoostingClassifier(n_estimators=50, \n",
    "                                           random_state=random_state), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=100, n_jobs=-1, \n",
    "                                     random_state=random_state),\n",
    "        'KNN' : KNeighborsClassifier(n_neighbors=30)}\n",
    "\n",
    "#predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "\n",
    "print('Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "   \n",
    "for nm, clf in clfs.items():\n",
    "    #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "    clf.fit(X_train, y_train)\n",
    "    yv = clf.predict_proba(X_valid)\n",
    "    p_valid.append(yv)\n",
    "        \n",
    "    #Second run. Training on (X, y) and predicting on X_test.\n",
    "    clf.fit(X, y)\n",
    "    yt = clf.predict_proba(X_test)\n",
    "    p_test.append(yt)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-1ad8f0cde86b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Creating the data for the 2nd layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mXV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mXT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print('Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)  \n",
    "        \n",
    "#EN_optA\n",
    "enA = EN_optA(n_classes)\n",
    "enA.fit(XV, y_valid)\n",
    "w_enA = enA.w\n",
    "y_enA = enA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "#Calibrated version of EN_optA \n",
    "cc_optA = CalibratedClassifierCV(enA, method='isotonic')\n",
    "cc_optA.fit(XV, y_valid)\n",
    "y_ccA = cc_optA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "#EN_optB\n",
    "enB = EN_optB(n_classes) \n",
    "enB.fit(XV, y_valid)\n",
    "w_enB = enB.w\n",
    "y_enB = enB.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB = CalibratedClassifierCV(enB, method='isotonic')\n",
    "cc_optB.fit(XV, y_valid)\n",
    "y_ccB = cc_optB.predict_proba(XT)  \n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.20%\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "y_pred = [round(i) for i in y_pred]\n",
    "accu = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: %.2f%%' % (accu * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.622029\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.560926\n",
      "[2]\tvalidation_0-logloss:0.51044\n",
      "[3]\tvalidation_0-logloss:0.466349\n",
      "[4]\tvalidation_0-logloss:0.427965\n",
      "[5]\tvalidation_0-logloss:0.395316\n",
      "[6]\tvalidation_0-logloss:0.366427\n",
      "[7]\tvalidation_0-logloss:0.339998\n",
      "[8]\tvalidation_0-logloss:0.317613\n",
      "[9]\tvalidation_0-logloss:0.294214\n",
      "[10]\tvalidation_0-logloss:0.273956\n",
      "[11]\tvalidation_0-logloss:0.256348\n",
      "[12]\tvalidation_0-logloss:0.240852\n",
      "[13]\tvalidation_0-logloss:0.22715\n",
      "[14]\tvalidation_0-logloss:0.215964\n",
      "[15]\tvalidation_0-logloss:0.205007\n",
      "[16]\tvalidation_0-logloss:0.196316\n",
      "[17]\tvalidation_0-logloss:0.187816\n",
      "[18]\tvalidation_0-logloss:0.180706\n",
      "[19]\tvalidation_0-logloss:0.172193\n",
      "[20]\tvalidation_0-logloss:0.164814\n",
      "[21]\tvalidation_0-logloss:0.158907\n",
      "[22]\tvalidation_0-logloss:0.15382\n",
      "[23]\tvalidation_0-logloss:0.149193\n",
      "[24]\tvalidation_0-logloss:0.145142\n",
      "[25]\tvalidation_0-logloss:0.141066\n",
      "[26]\tvalidation_0-logloss:0.13812\n",
      "[27]\tvalidation_0-logloss:0.134871\n",
      "[28]\tvalidation_0-logloss:0.130666\n",
      "[29]\tvalidation_0-logloss:0.126955\n",
      "[30]\tvalidation_0-logloss:0.124086\n",
      "[31]\tvalidation_0-logloss:0.120198\n",
      "[32]\tvalidation_0-logloss:0.116581\n",
      "[33]\tvalidation_0-logloss:0.114333\n",
      "[34]\tvalidation_0-logloss:0.111472\n",
      "[35]\tvalidation_0-logloss:0.109822\n",
      "[36]\tvalidation_0-logloss:0.109126\n",
      "[37]\tvalidation_0-logloss:0.10648\n",
      "[38]\tvalidation_0-logloss:0.104242\n",
      "[39]\tvalidation_0-logloss:0.102189\n",
      "[40]\tvalidation_0-logloss:0.100911\n",
      "[41]\tvalidation_0-logloss:0.100693\n",
      "[42]\tvalidation_0-logloss:0.098743\n",
      "[43]\tvalidation_0-logloss:0.097054\n",
      "[44]\tvalidation_0-logloss:0.096799\n",
      "[45]\tvalidation_0-logloss:0.096913\n",
      "[46]\tvalidation_0-logloss:0.096105\n",
      "[47]\tvalidation_0-logloss:0.094576\n",
      "[48]\tvalidation_0-logloss:0.095042\n",
      "[49]\tvalidation_0-logloss:0.095233\n",
      "[50]\tvalidation_0-logloss:0.094941\n",
      "[51]\tvalidation_0-logloss:0.09507\n",
      "[52]\tvalidation_0-logloss:0.095093\n",
      "[53]\tvalidation_0-logloss:0.095045\n",
      "[54]\tvalidation_0-logloss:0.094884\n",
      "[55]\tvalidation_0-logloss:0.093978\n",
      "[56]\tvalidation_0-logloss:0.094319\n",
      "[57]\tvalidation_0-logloss:0.093222\n",
      "[58]\tvalidation_0-logloss:0.092756\n",
      "[59]\tvalidation_0-logloss:0.09314\n",
      "[60]\tvalidation_0-logloss:0.093008\n",
      "[61]\tvalidation_0-logloss:0.093407\n",
      "[62]\tvalidation_0-logloss:0.09315\n",
      "[63]\tvalidation_0-logloss:0.092533\n",
      "[64]\tvalidation_0-logloss:0.092077\n",
      "[65]\tvalidation_0-logloss:0.092779\n",
      "[66]\tvalidation_0-logloss:0.093037\n",
      "[67]\tvalidation_0-logloss:0.093387\n",
      "[68]\tvalidation_0-logloss:0.093149\n",
      "[69]\tvalidation_0-logloss:0.092665\n",
      "[70]\tvalidation_0-logloss:0.093224\n",
      "[71]\tvalidation_0-logloss:0.093364\n",
      "[72]\tvalidation_0-logloss:0.09304\n",
      "[73]\tvalidation_0-logloss:0.093494\n",
      "[74]\tvalidation_0-logloss:0.093561\n",
      "Stopping. Best iteration:\n",
      "[64]\tvalidation_0-logloss:0.092077\n",
      "\n",
      "Accuracy: 97.20%\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "evaluation_set = [(X_test, y_test)]\n",
    "xgb.fit(X_train, y_train, early_stopping_rounds=10, eval_set=evaluation_set, eval_metric='logloss', verbose = True)\n",
    "y_pred = xgb.predict(X_test)\n",
    "y_pred = [round(i) for i in y_pred]\n",
    "accu = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: %.2f%%' % (accu * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d8acfeccc0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Learning Rate\n",
    "2. Tree\n",
    "    * max_depth\n",
    "    * min_child_weight\n",
    "    * subsample, colsample_bytree\n",
    "    * gamma\n",
    "3. Regularization\n",
    "    * lambda\n",
    "    * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "139px",
    "width": "368px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
